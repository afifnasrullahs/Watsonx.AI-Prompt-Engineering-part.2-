{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n\n# Implement a simple RAG use case with LangChain\n\n_Retrieval Augmented Generation (RAG)_ allows us to use LLMs to interact with \"external data\" i.e. data that was not used for model training. Many use cases require working with proprietary company data, and it's one of the reasons why RAG is frequently used in generative AI applications.\n\nThere is more than one way to implement the RAG pattern, which we will cover in a later lab. In this notebook, we will use _LangChain's RetrievalQA_ API to demonstrate one implementation of a RAG pattern. In general, RAG can be used for more than just question-and-answer use cases, but as you can tell from the name of the API, _RetrievalQA_ was implemented specifically for question-and-answer. \n\nTo get started we'll first verify that you have the necessary dependencies installed to run this notebook.\n\nGo ahead and run the following code cell. **This may take a few seconds to complete.**\n"}, {"metadata": {}, "cell_type": "code", "source": "# Install dependencies\n\nimport sys\n%pip install SQLAlchemy==2.0.29\n!{sys.executable} -m pip install -q ibm_watson_machine_learning==1.0.342\n!{sys.executable} -m pip install -q chromadb==0.4.22\n!{sys.executable} -m pip install -q langchain==0.1.4\n!{sys.executable} -m pip install -q pypdf==4.0.1\n!{sys.executable} -m pip install -q sentence-transformers\n\n# !{sys.executable} -m pip install -q chardet\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Collecting SQLAlchemy==2.0.29\n  Downloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy==2.0.29) (4.11.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from SQLAlchemy==2.0.29) (2.0.1)\nDownloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: SQLAlchemy\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 1.4.39\n    Uninstalling SQLAlchemy-1.4.39:\n      Successfully uninstalled SQLAlchemy-1.4.39\nSuccessfully installed SQLAlchemy-2.0.29\nNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nibm-watsonx-ai 0.2.6 requires ibm-watson-machine-learning>=1.0.349, but you have ibm-watson-machine-learning 1.0.342 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-ibm 0.1.4 requires langchain-core<0.2.0,>=0.1.42, but you have langchain-core 0.1.23 which is incompatible.\nlangchain-text-splitters 0.0.1 requires langchain-core<0.2.0,>=0.1.28, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Bring in dependencies\n\nIn this next code cell we'll bring in all the dependencies we'll need for later use.\n\nGo ahead and run the following code cell. **There should be no ouput.**"}, {"metadata": {}, "cell_type": "code", "source": "# Bring in dependencies\n# SQLite fix: https://docs.trychroma.com/troubleshooting#sqlite\n# __import__('pysqlite3')\n# import sys\n# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n\nfrom langchain.document_loaders.pdf import PyPDFLoader\nfrom langchain.chains import RetrievalQA\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\n\n# WML python SDK\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n## Some important variables\n\nIn this next code cell you'll define some variables that will be used in order to interact with your instance of watsonx.ai.\n\nGo ahead and run the following code cell. **There should be no ouput**"}, {"metadata": {}, "cell_type": "code", "source": "\n# Update the global variables that will be used for authentication in another function\nwatsonx_project_id = \"PASTE_PROJECT_ID_HERE\"\napi_key = \"PASTE_API_KEY_HERE\"\nurl = \"https://us-south.ml.cloud.ibm.com\"\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Understanding the code\n\nIn this next code cell we'll create some functions that we can use later to interact easier with watsonx.ai. These functions are ***get_model()***, ***get_lang_chain_model()***, and ***answer_question_from_doc()***:\n\n- ***get_model()***: creates a model object that will be used to invoke the LLM. Since the ***get_model()*** function is parametrized, it's the same in all examples.\n- ***get_lang_chain_model()***: creates a model wrapper that will be used with the _LangChain_ API.\n- ***answer_question_from_doc()*** specifies model parameters, loads the PDF file, creates an index from the loaded document, the instantiates and invokes the chain.\n\nGo ahead and run the following code cell. **There should be no ouput**."}, {"metadata": {}, "cell_type": "code", "source": "def get_model(model_type,max_tokens,min_tokens,decoding,temperature):\n\n    generate_params = {\n        GenParams.MAX_NEW_TOKENS: max_tokens,\n        GenParams.MIN_NEW_TOKENS: min_tokens,\n        GenParams.DECODING_METHOD: decoding,\n        GenParams.TEMPERATURE: temperature\n    }\n\n    model = Model(\n        model_id=model_type,\n        params=generate_params,\n        credentials={\n            \"apikey\": api_key,\n            \"url\": url\n        },\n        project_id=watsonx_project_id\n    )\n\n    return model\n\ndef get_lang_chain_model(model_type,max_tokens,min_tokens,decoding,temperature):\n\n    base_model = get_model(model_type,max_tokens,min_tokens,decoding,temperature)\n    langchain_model = WatsonxLLM(model=base_model)\n\n    return langchain_model\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Gluing it together\n\nThe next function, `answer_questions_from_doc`, that we create is created to help combine the previous three that we defined. This is the wrapper that we will call when we want to interact with watsonx.ai.\n\nGo ahead and run the following code cell. **There should be no ouput**."}, {"metadata": {}, "cell_type": "code", "source": "def answer_questions_from_doc(file_path, question):\n\n  # Specify model parameters\n  model_type = \"meta-llama/llama-2-70b-chat\"\n  max_tokens = 300\n  min_tokens = 100\n  decoding = DecodingMethods.GREEDY\n  temperature = 0.7\n\n  # Get the watsonx model that can be used with LangChain\n  model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n\n  loaders = [PyPDFLoader(file_path)]\n\n  index = VectorstoreIndexCreator(\n      embedding=HuggingFaceEmbeddings(),\n      text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)).from_loaders(loaders)\n\n  chain = RetrievalQA.from_chain_type(llm=model,\n                                      chain_type=\"stuff\",\n                                      retriever=index.vectorstore.as_retriever(),\n                                      input_key=\"question\")\n\n  # Invoke the chain\n  response_text = chain.run(question)\n\n  # print model response\n  print(\"--------------------------------- Generated response -----------------------------------\")\n  print(response_text)\n  print(\"*********************************************************************************************\")\n\n  return response_text\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Answering some questions\n\nThe next code cell will use all the previous code we've created so far to source information from the input documents and ask a question about them using watsonx.ai (Notice the use of the `answer_questions_from_doc`).\n\nTo do so we'll pass in a question we want to ask, the PDF file we want to reference for said question, and finally the name of the collection where the embeddings of the file exist.\n\nNotice the commented questions as well? Feel free to uncomment these or create some or your own to ask\n\nGo ahead and run the next code cell. **You _will_ see output from this cell**"}, {"metadata": {}, "cell_type": "code", "source": "# Test answering questions based on the provided .pdf file\nquestion = \"What is Generative AI?\"\n# question = \"What does it take to build a generative AI model?\"\n# question = \"What are the limitations of generative AI models?\"\nfile_path = \"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/L4assets/watsonx.ai-Assets/Documents/Generative_AI_Overview.pdf\"\n\nanswer_questions_from_doc(file_path, question)\n", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}